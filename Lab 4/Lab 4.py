# -*- coding: utf-8 -*-
"""Backpropagation Class Example.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nm48JR2CLhT_d9KAAGvujTnDj4oam9xU
"""

# Import Libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sympy import *

# Net Calculation
def Z_op(X,W,B):
    Z = np.dot(X,W.T) + B
    return Z

# Actual Output Calculation
def Y_op(Tf,Z):
    Yout = []
    for z in Z:
        y = Tf(z)
        Yout.append(y)
    return np.array(Yout)
    
# Error Calculation
def Cost(Y,T):
    errors = T - Y
    # Sum of sq err
    SSE = 0
    for error in errors:
        SSE = SSE + error*error
    # Mean sq err
    MSE = SSE/len(errors)
    return MSE

def feed_forward(W,B,X,Tf_list):
    Z_list = []
    X_list = []
    X_list.append(X)
    for i,j,k in zip(range(len(W)),range(len(B)),range(len(Tf_list))):
        z = Z_op(X,W[i],B[j])
        Z_list.append(z)
        # Using Transfer Function from the TF list 
        y = Y_op(Tf_list[k],z)
        # Equating actual output to next neuron input
        X = y
        X_list.append(X)
    # Return actual output and net
    return Z_list,X_list

def sensitivity(Z_list,X_list,F_prime,Target,Weights):
    F_list = []
    Sn_list = []
    # To change the shape of vector from (n,) -> (n,1)
    def reverse_reshaping(vector):
      if vector.shape == (len(vector),):
        return vector.reshape(len(vector),1)
      else :
        return vector
    for i in range(len(Z_list)):
      # F dot diagonal matrics calculation
        D = []
        for k in Z_list[i]:
            d = F_prime[i](k)
            D.append(d)
        Dag = np.array(D)
        F_dot = np.diag(Dag)
        F_list.append(F_dot)
    # Assigning F_n to last layer F dot value
    F_n = F_list.pop(len(F_list)-1)
    A_n = X_list.pop(len(X_list)-1)
    # Calculation of Error for calculation of last layer senditivity
    error = Target - A_n
    # Calculation of sensitivity of last layer of neural network
    S_n = np.array(-2*np.dot(F_n,error.T))
    Sn_list.append(S_n)
    # Calculation of previous layer sensitivities
    for i in range(len(F_list),0,-1):
        # F_dot_W = multiplication of Fn and Wn+1 transpose
        F_dot_W = np.array(np.dot(F_list[i-1],Weights[i].T))
        S_n = np.dot(reverse_reshaping(F_dot_W),reverse_reshaping(S_n))
        Sn_list.append(S_n.T)
    # Return List of Sensitivity indices
    return Sn_list

def weight_bias_updation(Weight,Bias,Alpha,Sensitivity,X_List):
    new_weights = []
    new_bias = []
    # To change the vector shape from (n,1) -> (n,)
    def reshaping(vector):
      if type(vector[0]) == np.float64:
        return vector
      else:
        if vector.shape == (len(vector),1):
          return vector.reshape(len(vector),)
        elif vector.shape == (1,len(vector[0])):
          return vector.reshape(len(vector[0]),)
        else :
          return vector
    # To change the shape of vector from (n,) -> (n,1)
    def reverse_reshaping(vector):
      if type(vector) == float:
        return vector
      else:
        if vector.shape == ():
          return vector.reshape(1,1)
        elif vector.shape == (len(vector),):
          return vector.reshape(len(vector),1)
        else :
          return vector
    # Reversing the Sensitivity List
    def Reverse(lst):
        return [ele for ele in reversed(lst)]
    sens = Reverse(Sensitivity)
    # Weight Updation
    for i in range(len(Weight)):
        # Using reshapeing and reverse reshaping for safe check the (n,1) and (n,) cases
        delta_w = reshaping(Alpha*sens[i]*reverse_reshaping(X_List[i]))
        w = Weight[i] - delta_w.T #As a(1) = X[0], The loop will run for w[0],w[1]...w[n-1], which we will use as w1,w2...wn
        new_weights.append(w)
    # Bias Updation
    for i in range(len(Bias)):
        b = Bias[i] - reshaping(Alpha*sens[i].T)
        new_bias.append(b)
    return new_weights,new_bias

def Error_Collection(Target,Output,All_Errors):
  Error = Cost(Output,Target)
  All_Errors.append(Error)

def backpropagation(Weight,Bias,Pattern,Target,Alpha,Transfer_Function,F_Prime,Epoch):
  all_errors = []
  epoch_list = []
  # Loop through number of epochs
  for i in range(Epoch):
      # Feed Forward
      zl,xl = feed_forward(Weight,Bias,Pattern,Transfer_Function)
      actual_output = xl[len(xl)-1]
      # Error Calculation
      Error_Collection(Target,actual_output,all_errors)
      # Sensitivity
      sen = sensitivity(zl,xl,F_Prime,Target,Weight)
      # Weight Bias Updation
      Weight,Bias = weight_bias_updation(Weight,Bias,Alpha,sen,xl)
      epoch_list.append(i+1)
  # Plot Epoch vs MSE diagram
  plt.scatter(epoch_list,all_errors)
  plt.xlabel('Number of Epoch')
  plt.ylabel('Mean Square Error')

def input_velues():
  # Getting user input
  print('Enter number of layers :')
  num_layers = int(input())
  num_nurons = []
  Tf_list = []
  F_prime = []
  W = []
  B = []
  X = []
  T = []
  print('\n')
  for num in range(num_layers):
    print('Enter numbers of neurons in layer {number} :'.format(number = num))
    num_nuron = int(input())
    num_nurons.append(num_nuron)
  print('Enter mathametical expression with respect to variable x.\n The variable should be continous, differntiable and use the proper Latex notation for each expression.\n')
  for num in range(num_layers - 1):
    # Converting Tf expression into function
    f_exp = input('Enter Expression for transfer function of layer {number} :'.format(number = num + 1))
    x = Symbol('x')
    f = lambdify(x,f_exp)
    # Differentiation of the Given Function  
    f_prime_exp = diff(f_exp)
    f_prime = lambdify(x,f_prime_exp)
    # Appending to the Transfer function and Derivative of Tf list
    Tf_list.append(f)
    F_prime.append(f_prime)

  def reshaping(vector):
    # To change the vector shape from (n,1) -> (n,)
    if type(vector[0]) == np.float64:
      return vector
    else:
      if vector.shape == (len(vector),1):
        return vector.reshape(len(vector),)
      elif vector.shape == (1,len(vector[0])):
        return vector.reshape(len(vector[0]),)
      else :
        return vector
  # Generating the random weight and bias
  for i in range(len(num_nurons)-1):
    w = np.random.rand(num_nurons[i+1],num_nurons[i])
    b = np.random.rand(num_nurons[i+1])
    W.append(w)
    B.append(b)
  new_w = []
  new_b = []
  # Reshaping the weight and bias matrics
  for w in W:
    w = reshaping(w)
    new_w.append(w)
  for b in B:
    b = reshaping(b)
    new_b.append(b)
  W = new_w
  B = new_b
  print('\n')
  print('Enter the list of patterns:')
  # Converting input string to point the input variable present in the environment
  pattern_input = input()
  X = globals()[pattern_input]
  print('Enter the target:')
  # Converting input string to point the target variable present in the environment
  target_input = input()
  T = globals()[target_input]
  X = np.array(X)
  T = np.array(T)
  # Input of Learning Rate and Epoch
  alpha = float(input('Enter the learning rate :'))
  epoch = int(input('Enter the number of epoch :'))
  return W,B,Tf_list,F_prime,X,T,alpha,epoch

def main():
  # Getting the input
  W,B,Tf_list,F_prime,X,Target,alpha,epoch = input_velues()
  # Print initial Weight and Bias 
  print('\nInitial Weights:')
  print(W)
  print('\nInitial Biases:')
  print(B)
  print('\n')
  # Backpropagetion
  backpropagation(W,B,X,Target,alpha,Tf_list,F_prime,epoch)

def g(p):
  return 1 + np.sin(np.pi /4 * p)

inp = np.random.uniform(-2,2,)
inp

out = g(inp)
out

main()